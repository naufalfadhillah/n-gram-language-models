{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Assignment: Exercises on n-gram language models, spelling correction, text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random, requests, nltk, pickle, operator, gc, string\n",
    "from distutils.util import strtobool\n",
    "from math import log2\n",
    "from nltk.corpus import PlaintextCorpusReader, stopwords\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.util import ngrams, pad_sequence\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends, flatten\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter, defaultdict\n",
    "from kneser_ney import KneserNeyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'output'\n",
    "file = '../europarl-v7.el-en.en'\n",
    "out1 = 'europarl-v7.el-en.en.train'\n",
    "out2 = 'europarl-v7.el-en.en.test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "    session = requests.Session()\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_corpus():\n",
    "    file_id = '1bowXfgxnMsL1OCV_QX-CSXm5XFsdvX2W'\n",
    "    destination = 'europarl-v7.el-en.en'\n",
    "    print('Downloading corpus...')\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "    print('Corpus downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file(file, out1, out2, percentage=0.75, isShuffle=True, seed=123):\n",
    "    \"\"\"Splits a file in 2 given the `percentage` to go in the large file.\"\"\"\n",
    "    random.seed(seed)\n",
    "    with open(file, 'r',encoding=\"utf-8\") as fin, \\\n",
    "         open(out1, 'w', encoding=\"utf-8\") as foutBig, \\\n",
    "         open(out2, 'w', encoding=\"utf-8\") as foutSmall:\n",
    "                nLines = sum(1 for line in fin)\n",
    "                fin.seek(0)\n",
    "                nTrain = int(nLines*percentage) \n",
    "                nValid = nLines - nTrain\n",
    "                i = 0\n",
    "                for line in fin:\n",
    "                    r = random.random() if isShuffle else 0 # so that always evaluated to true when not isShuffle\n",
    "                    if (i < nTrain and r < percentage) or (nLines - i > nValid):\n",
    "                        foutBig.write(line)\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        foutSmall.write(line)\n",
    "    print('Corpus splitted in train, test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(dirname=dirname):\n",
    "    # Create target Directory if it doesn't exist\n",
    "    if not os.path.split(os.path.abspath('.'))[1]==dirname and not os.path.exists(dirname):\n",
    "        if not os.path.exists(dirname):\n",
    "            os.mkdir(dirname)\n",
    "            os.chdir(dirname)\n",
    "            print(\"Directory\" , dirname ,  \"created.\")\n",
    "        else:\n",
    "            os.chdir(dirname)\n",
    "            print(\"Directory\" , dirname ,  \"already exists.\")\n",
    "    else:\n",
    "        #os.chdir(dirname)\n",
    "        print(\"Directory\" , dirname ,  \"already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    f = open('sentences_train_processed.pickle', 'rb')\n",
    "    sentences_train_processed = pickle.load(f)\n",
    "    f.close()\n",
    "    f = open('sentences_test_processed.pickle', 'rb')\n",
    "    sentences_test_processed = pickle.load(f)\n",
    "    f.close()\n",
    "    vocabulary = nltk.FreqDist(flatten(sentences_train_processed))\n",
    "    print('Preprosecced corpus loaded.')\n",
    "    return sentences_train_processed, sentences_test_processed, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    f = open('bigram_laplace.pickle', 'rb')\n",
    "    bigram_laplace = pickle.load(f)\n",
    "    f.close()\n",
    "    f = open('trigram_laplace.pickle', 'rb')\n",
    "    trigram_laplace = pickle.load(f)\n",
    "    f.close()\n",
    "    f = open('bigram_kneser.pickle', 'rb')\n",
    "    bigram_kneser = pickle.load(f)\n",
    "    f.close()\n",
    "    f = open('trigram_kneser.pickle', 'rb')\n",
    "    trigram_kneser = pickle.load(f)\n",
    "    f.close()\n",
    "    print('Models loaded.')\n",
    "    return bigram_laplace, trigram_laplace, bigram_kneser, trigram_kneser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_dataset(train, test):\n",
    "    f = open('sentences_train_processed.pickle', 'wb')\n",
    "    pickle.dump(train, f)\n",
    "    f.close()\n",
    "    f = open('sentences_test_processed.pickle', 'wb')\n",
    "    pickle.dump(test, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models():\n",
    "    f1 = open('bigram_laplace.pickle', 'wb')\n",
    "    pickle.dump(bigram_laplace, f1)\n",
    "    f1.close()\n",
    "    f2 = open('trigram_laplace.pickle', 'rb')\n",
    "    pickle.dump(trigram_laplace, f1)\n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt(question):\n",
    "    \"\"\" Prompts a Yes/No questions. \"\"\"\n",
    "    while True:\n",
    "        sys.stdout.write(question + \" [y/n]: \")\n",
    "        user_input = input().lower()\n",
    "        try:\n",
    "            result = strtobool(user_input)\n",
    "            return result\n",
    "        except ValueError:\n",
    "            sys.stdout.write(\"Please use y/n or yes/no.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(sentence, n, left_pad='*start*', right_pad='*end*'):\n",
    "    return (n-1)*[left_pad]+sentence+[right_pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sentence generator. Returns random (correct) sentences from the test subset or (incorrect) sentences of \n",
    "# the same length (in words) consisting of randomly selected vocabulary words.\n",
    "def my_sentences(sent_number=3, sent_len=7, random_select=False):\n",
    "    sentences = []\n",
    "    if random_select:\n",
    "        for _ in range(sent_number):\n",
    "            text = []\n",
    "            for _ in range(sent_len):\n",
    "                word=random.choice(list(vocabulary))\n",
    "                text.append(word)\n",
    "            sentences.append(text)\n",
    "    else:\n",
    "        for i in range(sent_number):\n",
    "            sentence = random.choice([sent for sent in test if len(sent)==sent_len])\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_probability(word):\n",
    "    if word in vocabulary:\n",
    "        unigram_probability = vocabulary[word]/vocabulary.N()\n",
    "        message = ''\n",
    "    else:\n",
    "        unigram_probability = vocabulary['*UNK*']/vocabulary.N()\n",
    "        message = '<<-- OOV word'\n",
    "    return(unigram_probability,message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram model with Laplace smoothing\n",
    "def unigram_laplace(word):\n",
    "        return (vocabulary[word]+1)/(vocabulary.N()+len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_corpus(cut_off = 10, test=True):\n",
    "    #os.chdir(dirname)\n",
    "\n",
    "    if os.path.isfile('sentences_train_processed.pickle') and os.path.isfile('sentences_test_processed.pickle'):\n",
    "        print('Corpus is already preprocessed.')\n",
    "        answer = user_prompt('Do you want to re-preprocess corpus?')\n",
    "        if answer == 0:\n",
    "            print('Loading preprocessed corpus...')\n",
    "            #os.chdir('..')\n",
    "            return load_dataset()\n",
    "        \n",
    "    corpus_root = '.'\n",
    "    corpus = PlaintextCorpusReader(corpus_root, '.*')\n",
    "    sentences_train = corpus.sents('europarl-v7.el-en.en.train')\n",
    "    sentences_test = corpus.sents('europarl-v7.el-en.en.test')\n",
    "\n",
    "    if test is True:\n",
    "        lines = 100\n",
    "    else:\n",
    "        lines = None\n",
    "        \n",
    "    print('Processing corpus...')\n",
    "    print('Lowercasing...')\n",
    "    words_train = list(flatten([[word.lower() for word in sent] for sent in sentences_train[:lines]]))\n",
    "    #words_train = list(flatten([[word.lower() for word in sent if word.lower() not in stopwords and word not in string.punctuation] for sent in sentences_train]))\n",
    "\n",
    "    freq_1gram = nltk.FreqDist(words_train)\n",
    "    total_count = freq_1gram.N()\n",
    "\n",
    "    #Some statistics about corpus\n",
    "    print('Number of sentences:', len(sentences_train[:lines]))\n",
    "    print('Number of tokens in corpus:', total_count)\n",
    "    print('Corpus Vocabulary length:', len(freq_1gram))\n",
    "\n",
    "    #Make vocabulary with cut-off value, replace OOV words in train-test set\n",
    "    print('Making vocabulary with cut-off value, replacing OOV words in train-test set...')\n",
    "    sentences_train_processed = [['*UNK*' if freq_1gram[word.lower()]<cut_off else word.lower() for word in sent] for sent in sentences_train[:lines]]\n",
    "    sentences_test_processed = [['*UNK*' if freq_1gram[word.lower()]<cut_off else word.lower() for word in sent] for sent in sentences_test[:lines]]\n",
    "    vocabulary = nltk.FreqDist(flatten(sentences_train_processed))\n",
    "    print('Vocabulary length:', len(vocabulary)) #after cut-off OOV words\n",
    "    save_processed_dataset(sentences_train_processed, sentences_test_processed)\n",
    "    print('Preprocessed corpus saved!')\n",
    "    #os.chdir('..')\n",
    "    return sentences_train_processed, sentences_test_processed, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_tests():\n",
    "    ## Get number of OOV words\n",
    "    ## and also compute unigram probabilities for a test sentence including OOV words\n",
    "    print('There are',vocabulary['*UNK*'],'OOV words.\\n')\n",
    "    print('The 10 most common vocabulary words:')\n",
    "    print(vocabulary.most_common()[:10],'\\n')\n",
    "    test_sentence = 'I admit that, At present, the matter seems to be somwhat confused.\\n'\n",
    "    test_sentence_tokens = word_tokenize(test_sentence)\n",
    "\n",
    "    print('Unigram probabilities including OOV probabilities for sentence:')\n",
    "    print(test_sentence)\n",
    "    print('{0:10s} {1:10s}   {2:10s}'.format('word', 'probability', 'message'))\n",
    "    print(40*'=')\n",
    "\n",
    "    for word in test_sentence_tokens:\n",
    "        unigram_probability,message = get_unigram_probability(word)\n",
    "        print('{0:10s} {1:.3}   {2:10s}'.format(word, unigram_probability, message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models(sentences_train_processed):\n",
    "    #os.chdir(dirname)\n",
    "    if (os.path.isfile('bigram_laplace.pickle') and os.path.isfile('trigram_laplace.pickle') and os.path.isfile('bigram_kneser.pickle') and os.path.isfile('trigram_kneser.pickle')) is True:\n",
    "        print('Models are already trained. Do you want to retrain?')\n",
    "        answer = user_prompt('say')\n",
    "        if answer == 0:\n",
    "            print('Loading trained models...')\n",
    "            return load_models()\n",
    "        \n",
    "    ## Bigram model\n",
    "    print('Training bigram model with Laplace smoothing...')\n",
    "    eur_2grams = list(bigrams(flatten(pad(sent, n=2) for sent in sentences_train_processed)))\n",
    "    freq_2gram = nltk.FreqDist(eur_2grams)\n",
    "    cfreq_2gram = nltk.ConditionalFreqDist(eur_2grams)\n",
    "\n",
    "    ### Laplace smoothing\n",
    "    #We compute Conditional probability Distribution with Laplace smothing\n",
    "    #bigram_laplace = nltk.ConditionalProbDist(cfreq_2gram, nltk.MLEProbDist)\n",
    "    bigram_laplace = nltk.ConditionalProbDist(cfreq_2gram, nltk.LaplaceProbDist, bins=len(vocabulary))\n",
    "    \n",
    "    ## Trigram model\n",
    "    print('Training trigram model with Laplace smoothing...')\n",
    "    eur_3grams = list(trigrams(flatten(pad(sent, n=3) for sent in sentences_train_processed)))\n",
    "    condition_pairs = [((w1, w2), w3) for w1, w2, w3 in eur_3grams]\n",
    "    cfreq_3gram = nltk.ConditionalFreqDist(condition_pairs)\n",
    "\n",
    "    ### Laplace smoothing\n",
    "    trigram_laplace = nltk.ConditionalProbDist(cfreq_3gram, nltk.LaplaceProbDist, bins=len(vocabulary))     \n",
    "        \n",
    "    print('Training bigram model with Kneser-Ney smoothing...')\n",
    "    bigram_kneser = KneserNeyLM(2, eur_2grams)\n",
    "\n",
    "\n",
    "    print('Training trigram model with Kneser-Ney smoothing...')\n",
    "    trigram_kneser = KneserNeyLM(3, eur_3grams)\n",
    "\n",
    "    #trigram_kneser.score_sent(('This', 'is','a', 'sentence'))\n",
    "        \n",
    "    #save models\n",
    "    print('Saving all models...')\n",
    "    f = open('bigram_laplace.pickle', 'wb')\n",
    "    pickle.dump(bigram_laplace, f)\n",
    "    f.close()\n",
    "    f = open('trigram_laplace.pickle', 'wb')\n",
    "    pickle.dump(trigram_laplace, f)\n",
    "    f.close() \n",
    "    f = open('bigram_kneser.pickle', 'wb')\n",
    "    pickle.dump(bigram_kneser, f)\n",
    "    f.close()\n",
    "    f = open('trigram_kneser.pickle', 'wb')\n",
    "    pickle.dump(trigram_kneser, f)\n",
    "    f.close()\n",
    "    print('Models saved')\n",
    "    \n",
    "    #os.chdir('..')\n",
    "    return bigram_laplace, trigram_laplace, bigram_kneser, trigram_kneser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tests():\n",
    "        #Test laplace smoothed models\n",
    "        #Do some tests on bigram model\n",
    "        print('=== Some tests on bigram model \\n')\n",
    "        print('P(declare|session):', bigram_laplace['session'].prob('declare'),'\\n')\n",
    "\n",
    "        print('Words coming after \"declare\" and their probability:')\n",
    "        test_ngram = ('declare')\n",
    "        prob_sum = 0\n",
    "        for i in list(sorted(bigram_laplace[test_ngram].samples()))[:10]:\n",
    "            prob_sum += bigram_laplace[test_ngram].prob(i)\n",
    "            print(\"{0} {1} {2}\".format(test_ngram, i, bigram_laplace[test_ngram].prob(i)))\n",
    "        print('Total probability:',prob_sum, '\\n')\n",
    "        \n",
    "        # Generate sentence from bigram model\n",
    "        print('Generate 3 (20 world) sentences from bigram model:')\n",
    "        for i in range(3):\n",
    "            current = random.choice(list(vocabulary))  # choose a random starting word\n",
    "            text = [current]\n",
    "            for index in range(20):\n",
    "                next = bigram_laplace[current].generate() # generate next word from bigram model\n",
    "                current = next                \n",
    "                text.append(current)  \n",
    "            print(' '.join(text),'\\n')\n",
    "\n",
    "\n",
    "        #Do some tests on trigram model\n",
    "        print('=== Some tests on trigram model \\n')\n",
    "\n",
    "        print('P(resumption,of|the):', trigram_laplace[('resumption','of')].prob('the'),'\\n')\n",
    "\n",
    "        print('Words coming after \"resumption of\" and their probability:')\n",
    "        test_ngram = ('resumption','of')\n",
    "        prob_sum = 0\n",
    "        for i in list(sorted(trigram_laplace[test_ngram].samples()))[:10]:\n",
    "            prob_sum += trigram_laplace[test_ngram].prob(i)\n",
    "            print(\"{0}{1}:{2}\".format(test_ngram, i, trigram_laplace[test_ngram].prob(i)))\n",
    "        print('Total probability:',prob_sum, '\\n')\n",
    "        \n",
    "        # Generate sentence from trigram model\n",
    "        print('Generate 3 (20 world) sentences from trigram model:')\n",
    "        for index in range(3):\n",
    "            prev = random.choice(list(vocabulary))  # choose a random starting word\n",
    "            current = bigram_laplace[prev].generate()   # generate next word from bigram model\n",
    "            text = [current]\n",
    "            for index in range(20):\n",
    "                next = trigram_laplace[(prev, current)].generate() # generate next word from trigram model\n",
    "                prev, current = current, next                \n",
    "                text.append(current)  \n",
    "            print(' '.join(text),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_sent_prob(sentences):\n",
    "    print('probabilities of word bigrams')\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sent_prob = 0\n",
    "        print(i+1,'====',' '.join(sentence))\n",
    "        for j,k in list(ngrams(sentence, 2, pad_left = True, pad_right = True, right_pad_symbol='*end*', left_pad_symbol='*start*')):\n",
    "            sent_prob = sent_prob + log2(bigram_laplace[j].prob(k))\n",
    "        print('Sentence probability:',sent_prob)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_sent_prob(sentences):\n",
    "    print('probabilities of word trigrams')\n",
    "    for i, sentence in enumerate(sentences):\n",
    "            sent_prob = 0\n",
    "            print(i+1,'====',' '.join(sentence))\n",
    "            for j,k,l in list(ngrams(sentence, 3, pad_left = True, pad_right = True, right_pad_symbol='*end*', left_pad_symbol='*start*')):\n",
    "                sent_prob = sent_prob + log2(trigram_laplace[(j,k)].prob(l))\n",
    "            print('Sentence probability:',sent_prob)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(test, pstart=True):\n",
    "    entropy2 = 0\n",
    "    entropy2_kneser = 0\n",
    "    N = 0\n",
    "    eur_2grams_test = list(bigrams(flatten(pad(sent, n=2) for sent in test)))\n",
    "    eur_3grams_test = list(trigrams(flatten(pad(sent, n=3) for sent in test)))\n",
    "    for j,k in eur_2grams_test:\n",
    "        if pstart is True:\n",
    "            entropy2 += log2(bigram_laplace[j].prob(k))\n",
    "            entropy2_kneser += log2(-bigram_kneser.score_sent((j,k)))\n",
    "            N += 1\n",
    "        else:\n",
    "            if k != '*start*':\n",
    "                entropy2 += log2(bigram_laplace[j].prob(k))\n",
    "                entropy2_kneser += log2(-bigram_kneser.score_sent((j,k)))\n",
    "                N += 1\n",
    "    entropy2 = -entropy2/N\n",
    "    entropy2_kneser = entropy2_kneser/N\n",
    "\n",
    "    \n",
    "    entropy3 = 0\n",
    "    entropy3_kneser = 0\n",
    "    N2 = 0\n",
    "    for j,k,l in eur_3grams_test:\n",
    "        if pstart is True:\n",
    "            entropy3 += log2(trigram_laplace[(j,k)].prob(l))\n",
    "            entropy3_kneser += log2(-trigram_kneser.score_sent((j,k,l)))\n",
    "            N2 += 1\n",
    "        else:\n",
    "            if l != '*start*':\n",
    "                entropy3 += log2(trigram_laplace[(j,k)].prob(l))\n",
    "                entropy3_kneser += log2(-trigram_kneser.score_sent((j,k,l)))\n",
    "                N2 += 1\n",
    "    entropy3 = -entropy3/N2\n",
    "    entropy3_kneser = entropy3_kneser/N2\n",
    "    \n",
    "    #print('Entropy of bigram model:',float('%.4g' % entropy), float('%.4g' % entropy_kneser))\n",
    "    print('======= Perplexity =======')\n",
    "    print('{0:10s} {1:10s} {2:10s}'.format('Model','Laplace smooth', 'Kneser-Ney smooth'))\n",
    "    print('{0:10s} {1:10} {2:10}'.format('Bigram',float('%.4g' % pow(2.0, entropy2)), float('%.4g' % pow(2.0, entropy2_kneser))))\n",
    "    print('{0:10s} {1:10} {2:10}'.format('Trigram',float('%.4g' % pow(2.0, entropy3)), float('%.4g' % pow(2.0, entropy3_kneser))))\n",
    "\n",
    "    print()\n",
    "    print('======= Entropy =======')\n",
    "    print('{0:10s} {1:10s} {2:10s}'.format('Model','Laplace smooth', 'Kneser-Ney smooth'))\n",
    "    print('{0:10s} {1:10} {2:10}'.format('Bigram',float('%.4g' % entropy2), float('%.4g' % entropy2_kneser)))\n",
    "    print('{0:10s} {1:10} {2:10}'.format('Trigram',float('%.4g' % entropy3), float('%.4g' % entropy3_kneser)))\n",
    "    print()\n",
    "    print('Tested on', N, 'bigrams,', N2, 'trigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_interpolated(test, pstart=True):\n",
    "    entropy = 0\n",
    "    entropy_kneser = 0\n",
    "    N = 0\n",
    "    l1 = 2/10\n",
    "    l2 = 8/10\n",
    "    l3 = 1/10\n",
    "\n",
    "    eur_3grams_test = list(trigrams(flatten(pad(sent, n=3) for sent in test)))\n",
    "\n",
    "    for j,k,l in eur_3grams_test:\n",
    "        if pstart is True:\n",
    "            entropy += log2(l1*trigram_laplace[(j,k)].prob(l)+l2*(bigram_laplace[k].prob(l))+l3*unigram_laplace(l))\n",
    "            entropy_kneser += log2(-l1*trigram_kneser.score_sent((j,k,l))-l2*bigram_kneser.score_sent((j,k))+l3*unigram_laplace(l))\n",
    "            N += 1\n",
    "        else:\n",
    "            if l != '*start*':\n",
    "                entropy += log2(l1*trigram_laplace[(j,k)].prob(l)+l2*(bigram_laplace[k].prob(l))+l3*unigram_laplace(l))\n",
    "                entropy_kneser += log2(-l1*trigram_kneser.score_sent((j,k,l))-l2*bigram_kneser.score_sent((j,k))+l3*unigram_laplace(l))\n",
    "                N += 1\n",
    "    entropy = -entropy/N\n",
    "    entropy_kneser = entropy_kneser/N\n",
    "    \n",
    "    #print('Entropy of bigram model:',float('%.4g' % entropy), float('%.4g' % entropy_kneser))\n",
    "    print('======= Perplexity =======')\n",
    "    print('{0:10s} {1:10s} {2:10s}'.format('Model','Laplace smooth', 'Kneser-Ney smooth'))\n",
    "    print('{0:10s} {1:10} {2:10}'.format('Trigram',float('%.4g' % pow(2.0, entropy)), float('%.4g' % pow(2.0, entropy_kneser))))\n",
    "\n",
    "    print()\n",
    "    print('======= Entropy =======')\n",
    "    print('{0:10s} {1:10s} {2:10s}'.format('Model','Laplace smooth', 'Kneser-Ney smooth'))\n",
    "    print('{0:10s} {1:10} {2:10}'.format('Trigram',float('%.4g' % entropy), float('%.4g' % entropy_kneser)))\n",
    "    print()\n",
    "    print('Tested on', N, 'trigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== Exercice 4 ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download corpus, create output directory, split corpus in train, test set (defaults: percentage=0.75, isShuffle=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading corpus...\n",
      "Corpus downloaded.\n"
     ]
    }
   ],
   "source": [
    "download_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory output created.\n"
     ]
    }
   ],
   "source": [
    "create_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus splitted in train, test set\n"
     ]
    }
   ],
   "source": [
    "split_file(file, out1, out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Lowercase, replace OOV words in train-test set with *UNK* token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus...\n",
      "Lowercasing...\n",
      "Number of sentences: 986576\n",
      "Number of tokens in corpus: 27063156\n",
      "Corpus Vocabulary length: 64143\n",
      "Making vocabulary with cut-off value, replacing OOV words in train-test set...\n",
      "Vocabulary length: 22622\n",
      "Preprocessed corpus saved!\n"
     ]
    }
   ],
   "source": [
    "train, test, vocabulary = preprocess_corpus(test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some tests: Get number of OOV words, compute unigram probabilities for a test sentence including OOV words, print 10 most common vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 106424 OOV words.\n",
      "\n",
      "The 10 most common vocabulary words:\n",
      "[('the', 1836392), (',', 1281104), ('.', 950979), ('of', 880323), ('to', 805597), ('and', 710636), ('in', 581890), ('that', 426592), ('a', 403882), ('is', 401500)] \n",
      "\n",
      "Unigram probabilities including OOV probabilities for sentence:\n",
      "I admit that, At present, the matter seems to be somwhat confused.\n",
      "\n",
      "word       probability   message   \n",
      "========================================\n",
      "I          0.00393   <<-- OOV word\n",
      "admit      2.81e-05             \n",
      "that       0.0158             \n",
      ",          0.0473             \n",
      "At         0.00393   <<-- OOV word\n",
      "present    0.000338             \n",
      ",          0.0473             \n",
      "the        0.0679             \n",
      "matter     0.000404             \n",
      "seems      0.000144             \n",
      "to         0.0298             \n",
      "be         0.00754             \n",
      "somwhat    0.00393   <<-- OOV word\n",
      "confused   9.9e-06             \n",
      ".          0.0351             \n"
     ]
    }
   ],
   "source": [
    "do_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the padded bigrams, trigrams, compute the Frequency Distribution, Conditional Frequency Distribution, Conditional probability Distribution and train models with Laplace or Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training bigram model with Laplace smoothing...\n",
      "Training trigram model with Laplace smoothing...\n",
      "Training bigram model with Kneser-Ney smoothing...\n",
      "Training trigram model with Kneser-Ney smoothing...\n",
      "Saving all models...\n",
      "Models saved\n"
     ]
    }
   ],
   "source": [
    "bigram_laplace, trigram_laplace, bigram_kneser, trigram_kneser = build_models(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some tests on bigram, trigram models trained with laplace smoothing, like finding conditional probabilities of a word given its context and sentence generation from the relevant probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Some tests on bigram model \n",
      "\n",
      "P(declare|session): 3.9767756303189374e-05 \n",
      "\n",
      "Words coming after \"declare\" and their probability:\n",
      "declare ' 0.00017100594245650035\n",
      "declare *UNK* 0.00012825445684237527\n",
      "declare , 0.0003420118849130007\n",
      "declare . 8.550297122825018e-05\n",
      "declare 11 8.550297122825018e-05\n",
      "declare 14 8.550297122825018e-05\n",
      "declare 20 0.00012825445684237527\n",
      "declare 2007 0.00012825445684237527\n",
      "declare 2009 0.00012825445684237527\n",
      "declare 2010 0.00012825445684237527\n",
      "Total probability: 0.0014107990252661279 \n",
      "\n",
      "Generate 3 (20 world) sentences from bigram model:\n",
      "backed proposals faithfully transpose within unesco monument ( bankers : independence had led ones spending existing com now moving back , \n",
      "\n",
      "bielan , must oppose these restrictive eligibility period only giving permanent general , which is not conflict control until tonight will \n",
      "\n",
      "subservient . *end* *start* i wished at hand then buy ! *end* *start* everything *UNK* who cover throughout bilateral overseas budget \n",
      "\n",
      "=== Some tests on trigram model \n",
      "\n",
      "P(resumption,of|the): 0.007288756983240223 \n",
      "\n",
      "Words coming after \"resumption of\" and their probability:\n",
      "('resumption', 'of')a:0.00017458100558659218\n",
      "('resumption', 'of')aid:0.00026187150837988826\n",
      "('resumption', 'of')all:0.00013093575418994413\n",
      "('resumption', 'of')an:0.00026187150837988826\n",
      "('resumption', 'of')basic:8.729050279329609e-05\n",
      "('resumption', 'of')cohesion:8.729050279329609e-05\n",
      "('resumption', 'of')commercial:8.729050279329609e-05\n",
      "('resumption', 'of')constitutional:8.729050279329609e-05\n",
      "('resumption', 'of')contact:8.729050279329609e-05\n",
      "('resumption', 'of')contacts:8.729050279329609e-05\n",
      "Total probability: 0.001353002793296089 \n",
      "\n",
      "Generate 3 (20 world) sentences from trigram model:\n",
      ", which we presented . *end* *start* *start* i welcome pilar del castillo on the other peoples - serbs and also \n",
      "\n",
      "energy , about critical dialogue ' to appear next week zimbabwe is one single starting date early next december . the \n",
      "\n",
      "for euratom is going poorly and too ambitious , objectives for manufacturers , competent trade commissioner catherine ashton about increasing evidence \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II\n",
    "\n",
    "Check the log-probabilities that your trained models return when given (correct) sentences from the test subset vs. (incorrect) sentences of the same length (in words) consisting of randomly selected vocabulary words.  \n",
    "\n",
    "We use the Laplace smoothed models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram sentence probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities of word bigrams\n",
      "1 ==== i attempted to contact an official .\n",
      "Sentence probability: -71.38011548653269\n",
      "\n",
      "2 ==== i find such a remark disturbing .\n",
      "Sentence probability: -62.9145618750343\n",
      "\n",
      "3 ==== i do however take your point .\n",
      "Sentence probability: -58.58715226409559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram_sent_prob(my_sentences(random_select=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities of word bigrams\n",
      "1 ==== empire djibouti ritual crumbling hatzidakis affording raided\n",
      "Sentence probability: -121.22550844540186\n",
      "\n",
      "2 ==== anguish scientifically repay synchronisation bona jeopardised 863\n",
      "Sentence probability: -121.2281776495057\n",
      "\n",
      "3 ==== almadén opponent carries friendly fosters emigrants eye\n",
      "Sentence probability: -120.37230155918866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram_sent_prob(my_sentences(random_select=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram sentence probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities of word trigrams\n",
      "1 ==== mrs van *UNK* spoke about tibet .\n",
      "Sentence probability: -111.94015643004988\n",
      "\n",
      "2 ==== it was a very controversial amendment .\n",
      "Sentence probability: -85.27026810846932\n",
      "\n",
      "3 ==== we cannot therefore support paragraph 11 .\n",
      "Sentence probability: -98.01354122839139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trigram_sent_prob(my_sentences(random_select=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities of word trigrams\n",
      "1 ==== partial concentration sin pretend incomparably inaugurated pulses\n",
      "Sentence probability: -134.0834537631206\n",
      "\n",
      "2 ==== statehood aforesaid rejection requirements payback multiannual germà\n",
      "Sentence probability: -135.66828872151385\n",
      "\n",
      "3 ==== channelling carnage asleep regularisations hoc domiciled preoccupied\n",
      "Sentence probability: -134.66835249408723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trigram_sent_prob(my_sentences(random_select=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III - Entropy, Perplexity\n",
    "\n",
    "Estimate the language **cross-entropy** and **perplexity** of your models on the test subset of the corpus, treating the entire test subset as a single sequence, with *start* (or *start1*, *start2*) at the beginning of each sentence, and *end* at the end of each sentence. Do not include probabilities of the form P(*start*|...) (or P(*start1*|...) or P(*start2*|...)) in the computation of perplexity, but include probabilities of the form P(*end*|...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Perplexity =======\n",
      "Model      Laplace smooth Kneser-Ney smooth\n",
      "Bigram          171.5      14.26\n",
      "Trigram         678.5      17.97\n",
      "\n",
      "======= Entropy =======\n",
      "Model      Laplace smooth Kneser-Ney smooth\n",
      "Bigram          7.422      3.834\n",
      "Trigram         9.406      4.167\n",
      "\n",
      "Tested on 3337 bigrams, 3436 trigrams\n"
     ]
    }
   ],
   "source": [
    "perplexity(test[:100], pstart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Perplexity =======\n",
      "Model      Laplace smooth Kneser-Ney smooth\n",
      "Bigram          200.6      13.97\n",
      "Trigram        1008.0      17.28\n",
      "\n",
      "======= Entropy =======\n",
      "Model      Laplace smooth Kneser-Ney smooth\n",
      "Bigram          7.648      3.804\n",
      "Trigram         9.978      4.111\n",
      "\n",
      "Tested on 3238 bigrams, 3238 trigrams\n"
     ]
    }
   ],
   "source": [
    "perplexity(test[:100], pstart=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV - Linear interpolation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine unigram, bigram and trigram models using linear interpolation and check if the combined model performs better. Best l1, l2, l3 parameters found after some trials on a validation set of 100 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Perplexity =======\n",
      "Model      Laplace smooth Kneser-Ney smooth\n",
      "Trigram         139.5      15.34\n",
      "\n",
      "======= Entropy =======\n",
      "Model      Laplace smooth Kneser-Ney smooth\n",
      "Trigram         7.124       3.94\n",
      "\n",
      "Tested on 3436 trigrams\n"
     ]
    }
   ],
   "source": [
    "perplexity_interpolated(test[:100], pstart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Perplexity =======\n",
      "Model      Laplace smooth Kneser-Ney smooth\n",
      "Trigram         179.3      14.91\n",
      "\n",
      "======= Entropy =======\n",
      "Model      Laplace smooth Kneser-Ney smooth\n",
      "Trigram         7.486      3.898\n",
      "\n",
      "Tested on 3238 trigrams\n"
     ]
    }
   ],
   "source": [
    "perplexity_interpolated(test[:100], pstart=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======== ADDITIONAL WORK ========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Entropy and perplexity of LMs trained with NLTK's pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Nltk's pipeline to train LMs with maximum order 3, using Laplace smothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE, Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_train(sentences_train_processed):\n",
    "    if os.path.isfile('lm_nltk_laplace.pickle') is True:\n",
    "        print('Model trained with Nltk exist in your disk. Do you want to retrain?')\n",
    "        answer = user_prompt('say')\n",
    "        if answer == 0:\n",
    "            print('Loading trained model...')\n",
    "            f = open('lm_nltk_laplace.pickle', 'rb')\n",
    "            lm_nltk = pickle.load(f)\n",
    "            f.close()\n",
    "            print('Model loaded.')\n",
    "            return lm_nltk\n",
    "\n",
    "    print('Training unigram, bigram, trigram model with Laplace smoothing...')\n",
    "    train, vocab = padded_everygram_pipeline(3, sentences_train_processed)\n",
    "    lm_nltk = Laplace(3)\n",
    "    lm_nltk.fit(train, vocab)\n",
    "\n",
    "    f = open('lm_nltk_laplace.pickle', 'wb')\n",
    "    pickle.dump(lm_nltk, f)\n",
    "    f.close()\n",
    "    print('Model saved.')\n",
    "    return lm_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training unigram, bigram, trigram model with Laplace smoothing...\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "lm = nltk_train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22625"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 22625 items>\n"
     ]
    }
   ],
   "source": [
    "print(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'*UNK*' in lm.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48312"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.vocab['europe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.32713347750879"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.logscore('europe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.49741527084874"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.logscore('demonstrate', ['europe','can'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263.6186366559596"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eur_2grams_test = list(bigrams(flatten(pad_both_ends(sent, n=2) for sent in test[:100])))\n",
    "lm.perplexity(eur_2grams_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "988.670572284987"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eur_3grams_test = list(trigrams(flatten(pad_both_ends(sent, n=3) for sent in test[:100])))\n",
    "lm.perplexity(eur_3grams_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
